{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssef-Chip/lab1/blob/main/lab2/01_data_and_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5363c92",
      "metadata": {
        "id": "d5363c92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb8994c",
      "metadata": {
        "id": "8cb8994c"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/Users/mr.youssef/Dropbox/lab1/lab2/dataset/UCI_HAR_Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c9a722",
      "metadata": {
        "id": "f8c9a722"
      },
      "outputs": [],
      "source": [
        "def load_inertial_signals(subset):\n",
        "    signals = ['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z','total_acc_x', 'total_acc_y', 'total_acc_z']\n",
        "    loaded_signals = []\n",
        "    for sig in signals:\n",
        "        path = os.path.join(DATA_DIR, subset, 'Inertial Signals', f'{sig}_{subset}.txt')\n",
        "        loaded_signals.append(pd.read_csv(path, sep=r'\\s+', header=None).values)\n",
        "        # each file has shape: (N, 128) where N is the number of windows (2.56 seconds per window) and 128 is sequential time steps for that specific window\n",
        "    # Go from 9 arrays of: (N, 128) to 1 matrix (N, 128, 9)\n",
        "    return np.dstack(loaded_signals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9b6ae7",
      "metadata": {
        "id": "fd9b6ae7"
      },
      "outputs": [],
      "source": [
        "def load_y_and_subjects(subset):\n",
        "    # y ==> 0 to 5 multiclass describing the activity\n",
        "    # subjecs: each row has the ID of the specific human volunteer who was wearing the phone for that specific window\n",
        "    y_path = os.path.join(DATA_DIR, subset, f'y_{subset}.txt')\n",
        "    sub_path = os.path.join(DATA_DIR, subset, f'subject_{subset}.txt')\n",
        "    y = pd.read_csv(y_path, sep=r'\\s+', header=None).values.squeeze() - 1 # 0-indexed\n",
        "    subjects = pd.read_csv(sub_path, sep=r'\\s+', header=None).values.squeeze()\n",
        "    return y, subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a16ca302",
      "metadata": {
        "id": "a16ca302"
      },
      "outputs": [],
      "source": [
        "X_train = load_inertial_signals('train')\n",
        "X_test = load_inertial_signals('test')\n",
        "y_train, subjects_train = load_y_and_subjects('train')\n",
        "y_test, subjects_test = load_y_and_subjects('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dff09d0",
      "metadata": {
        "id": "3dff09d0",
        "outputId": "6336059c-65e5-40bd-b6d8-bbeb83e89c48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7352, 128, 9)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9deba912",
      "metadata": {
        "id": "9deba912",
        "outputId": "da1ce4c9-1f66-48ad-a798-b5d0ae27d144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2947, 128, 9)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8260c381",
      "metadata": {
        "id": "8260c381",
        "outputId": "677d8cb2-294a-460c-cd4a-fc54a12252ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7352,) (7352,)\n"
          ]
        }
      ],
      "source": [
        "print(y_train.shape, subjects_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9c0507",
      "metadata": {
        "id": "1c9c0507",
        "outputId": "deb858c5-c714-407f-b325-5e360195a919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2947,) (2947,)\n"
          ]
        }
      ],
      "source": [
        "print(y_test.shape, subjects_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cba9a0",
      "metadata": {
        "id": "76cba9a0",
        "outputId": "02831bfd-1aa1-473c-fe6a-cdb6d3b3228f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaNs: 0 and Infs: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"NaNs: {np.isnan(X_train).sum()} and Infs: {np.isinf(X_train).sum()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1bb8850",
      "metadata": {
        "id": "f1bb8850",
        "outputId": "aaaa51f9-dbf7-406c-b7f4-d219770b8130"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1226, 1073,  986, 1286, 1374, 1407])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.bincount(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af33f07",
      "metadata": {
        "id": "9af33f07",
        "outputId": "19f8f8f6-e156-441b-81c8-d379d7f80c50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533becbe",
      "metadata": {
        "id": "533becbe",
        "outputId": "9874c67f-a724-451b-aec1-8bbb782ed09f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  3,  5,  6,  7,  8, 11, 14, 15, 16, 17, 19, 21, 22, 23, 25, 26,\n",
              "       27, 28, 29, 30])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(subjects_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e366cf1",
      "metadata": {
        "id": "5e366cf1"
      },
      "outputs": [],
      "source": [
        "# leakage check\n",
        "np.random.seed(42)\n",
        "unique_train_subs = np.unique(subjects_train)\n",
        "# Hold out 4 subjects from the training pool for checking\n",
        "val_subs = np.random.choice(unique_train_subs, size=4, replace=False)\n",
        "# get everyone from the training pool instead the 4 subjects above\n",
        "train_subs = np.setdiff1d(unique_train_subs, val_subs)\n",
        "# Prove overlap is 0\n",
        "train_val_overlap = set(train_subs).intersection(set(val_subs))\n",
        "train_test_overlap = set(train_subs).intersection(set(subjects_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "978df990",
      "metadata": {
        "id": "978df990",
        "outputId": "12dd3372-019b-4326-9ea0-2c38122a44b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0\n"
          ]
        }
      ],
      "source": [
        "print(len(train_val_overlap), len(train_test_overlap))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MajorityClassPredictor:\n",
        "  \"\"\"\n",
        "  Trivial baseline model that always predicts the majority class.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.majorityClass = None\n",
        "\n",
        "  def fit(self, trainLabels):\n",
        "    \"\"\"\n",
        "    Method to find the most frequent class in the training labels\n",
        "    \"\"\"\n",
        "    values, count = np.unique(trainLabels, return_counts=True)\n",
        "    self.majorityClass = values[np.argmax(count)]\n",
        "\n",
        "  def predict(self, numSamples):\n",
        "    \"\"\"\n",
        "    Method to blindly return the majority class for every sample.\n",
        "    \"\"\"\n",
        "    return np.full(numSamples, self.majorityClass)"
      ],
      "metadata": {
        "id": "pdqLykZAuBBE"
      },
      "id": "pdqLykZAuBBE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0effogvGpE_K",
      "metadata": {
        "id": "0effogvGpE_K"
      },
      "outputs": [],
      "source": [
        "class BaselineMLP(nn.Module):\n",
        "  \"\"\"\n",
        "  Baseline MLP model for classification tasks.\n",
        "  - inputDim: number of input features\n",
        "  - hiddenDim: Our hyperparameter for the hidden layer. This defines the size of\n",
        "               the hidden layer.\n",
        "  - numClasses: number of output classes\n",
        "  \"\"\"\n",
        "  def __init__(self, inputDim, hiddenDim, numClasses):\n",
        "    super(BaselineMLP, self).__init__()\n",
        "\n",
        "    # Define the layers\n",
        "    # Layer 1: Input -> Hidden\n",
        "    self.layer1 = nn.Linear(inputDim, hiddenDim)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.drop1 = nn.Dropout(0.3) # turn off 30% of neurons everytime data comes throuhg\n",
        "    # Layer 2: Hidden -> Output\n",
        "    self.layer2 = nn.Linear(hiddenDim, numClasses)\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Method to perform forward propagation from:\n",
        "    input layer -> hidden layer -> output layer.\n",
        "\n",
        "    The 2D input X passes through the first layer, goes through ReLU activation,\n",
        "    introducing non-linearity, and then passes through the second layer.\n",
        "    \"\"\"\n",
        "    X = self.layer1(X)\n",
        "    X = self.activation(X)\n",
        "    X = self.drop1(X)\n",
        "    X = self.layer2(X)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a443786",
      "metadata": {
        "id": "2a443786"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(X, y, batch_size=64, shuffle=False):\n",
        "    # Flatten: (N, 128, 9) -> (N, 128*9)\n",
        "    X_flat = X.reshape(X.shape[0], -1)\n",
        "\n",
        "    # Convert to PyTorch Tensors\n",
        "    dataset = TensorDataset(\n",
        "        torch.tensor(X_flat, dtype=torch.float32),\n",
        "        torch.tensor(y, dtype=torch.long)\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_mask = np.isin(subjects_train, train_subs)\n",
        "val_mask = np.isin(subjects_train, val_subs)\n",
        "X_t, y_t = X_train[train_mask], y_train[train_mask]\n",
        "X_v, y_v = X_train[val_mask], y_train[val_mask]\n",
        "\n",
        "train_loader = create_dataloader(X_t, y_t, shuffle=True)\n",
        "val_loader = create_dataloader(X_v, y_v)\n",
        "test_loader = create_dataloader(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb03f5b",
      "metadata": {
        "id": "4cb03f5b",
        "outputId": "2cc03809-7b82-47a4-bf59-264f887bebaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Val Accuracy: 0.8717\n",
            "Epoch 2/100 | Val Accuracy: 0.9212\n",
            "Epoch 3/100 | Val Accuracy: 0.9355\n",
            "Epoch 4/100 | Val Accuracy: 0.9362\n",
            "Epoch 5/100 | Val Accuracy: 0.9416\n",
            "Epoch 6/100 | Val Accuracy: 0.9443\n",
            "Epoch 7/100 | Val Accuracy: 0.9443\n",
            "Epoch 8/100 | Val Accuracy: 0.9525\n",
            "Epoch 9/100 | Val Accuracy: 0.9470\n",
            "Epoch 10/100 | Val Accuracy: 0.9477\n",
            "Epoch 11/100 | Val Accuracy: 0.9464\n",
            "Epoch 12/100 | Val Accuracy: 0.9491\n",
            "Epoch 13/100 | Val Accuracy: 0.9464\n",
            "Epoch 14/100 | Val Accuracy: 0.9477\n",
            "Epoch 15/100 | Val Accuracy: 0.9416\n",
            "Epoch 16/100 | Val Accuracy: 0.9443\n",
            "Epoch 17/100 | Val Accuracy: 0.9423\n",
            "Epoch 18/100 | Val Accuracy: 0.9443\n",
            "Epoch 19/100 | Val Accuracy: 0.9362\n",
            "Epoch 20/100 | Val Accuracy: 0.9498\n",
            "Epoch 21/100 | Val Accuracy: 0.9504\n",
            "Epoch 22/100 | Val Accuracy: 0.9464\n",
            "Epoch 23/100 | Val Accuracy: 0.9538\n",
            "Epoch 24/100 | Val Accuracy: 0.9484\n",
            "Epoch 25/100 | Val Accuracy: 0.9511\n",
            "Epoch 26/100 | Val Accuracy: 0.9484\n",
            "Epoch 27/100 | Val Accuracy: 0.9498\n",
            "Epoch 28/100 | Val Accuracy: 0.9437\n",
            "Epoch 29/100 | Val Accuracy: 0.9477\n",
            "Epoch 30/100 | Val Accuracy: 0.9443\n",
            "Epoch 31/100 | Val Accuracy: 0.9477\n",
            "Epoch 32/100 | Val Accuracy: 0.9498\n",
            "Epoch 33/100 | Val Accuracy: 0.9348\n",
            "Epoch 34/100 | Val Accuracy: 0.9416\n",
            "Epoch 35/100 | Val Accuracy: 0.9430\n",
            "Epoch 36/100 | Val Accuracy: 0.9335\n",
            "Epoch 37/100 | Val Accuracy: 0.9464\n",
            "Epoch 38/100 | Val Accuracy: 0.9525\n",
            "Epoch 39/100 | Val Accuracy: 0.9518\n",
            "Epoch 40/100 | Val Accuracy: 0.9450\n",
            "Epoch 41/100 | Val Accuracy: 0.9450\n",
            "Epoch 42/100 | Val Accuracy: 0.9470\n",
            "Epoch 43/100 | Val Accuracy: 0.9450\n",
            "Epoch 44/100 | Val Accuracy: 0.9450\n",
            "Epoch 45/100 | Val Accuracy: 0.9464\n",
            "Epoch 46/100 | Val Accuracy: 0.9538\n",
            "Epoch 47/100 | Val Accuracy: 0.9403\n",
            "Epoch 48/100 | Val Accuracy: 0.9430\n",
            "Epoch 49/100 | Val Accuracy: 0.9538\n",
            "Epoch 50/100 | Val Accuracy: 0.9396\n",
            "Epoch 51/100 | Val Accuracy: 0.9450\n",
            "Epoch 52/100 | Val Accuracy: 0.9375\n",
            "Epoch 53/100 | Val Accuracy: 0.9470\n",
            "Epoch 54/100 | Val Accuracy: 0.9491\n",
            "Epoch 55/100 | Val Accuracy: 0.9504\n",
            "Epoch 56/100 | Val Accuracy: 0.9430\n",
            "Epoch 57/100 | Val Accuracy: 0.9464\n",
            "Epoch 58/100 | Val Accuracy: 0.9518\n",
            "Epoch 59/100 | Val Accuracy: 0.9498\n",
            "Epoch 60/100 | Val Accuracy: 0.9498\n",
            "Epoch 61/100 | Val Accuracy: 0.9491\n",
            "Epoch 62/100 | Val Accuracy: 0.9538\n",
            "Epoch 63/100 | Val Accuracy: 0.9538\n",
            "Epoch 64/100 | Val Accuracy: 0.9525\n",
            "Epoch 65/100 | Val Accuracy: 0.9477\n",
            "Epoch 66/100 | Val Accuracy: 0.9504\n",
            "Epoch 67/100 | Val Accuracy: 0.9470\n",
            "Epoch 68/100 | Val Accuracy: 0.9491\n",
            "Epoch 69/100 | Val Accuracy: 0.9457\n",
            "Epoch 70/100 | Val Accuracy: 0.9477\n",
            "Epoch 71/100 | Val Accuracy: 0.9348\n",
            "Epoch 72/100 | Val Accuracy: 0.9498\n",
            "Epoch 73/100 | Val Accuracy: 0.9450\n",
            "Epoch 74/100 | Val Accuracy: 0.9430\n",
            "Epoch 75/100 | Val Accuracy: 0.9470\n",
            "Epoch 76/100 | Val Accuracy: 0.9450\n",
            "Epoch 77/100 | Val Accuracy: 0.9484\n",
            "Epoch 78/100 | Val Accuracy: 0.9437\n",
            "Epoch 79/100 | Val Accuracy: 0.9464\n",
            "Epoch 80/100 | Val Accuracy: 0.9477\n",
            "Epoch 81/100 | Val Accuracy: 0.9511\n",
            "Epoch 82/100 | Val Accuracy: 0.9457\n",
            "Epoch 83/100 | Val Accuracy: 0.9416\n",
            "Epoch 84/100 | Val Accuracy: 0.9470\n",
            "Epoch 85/100 | Val Accuracy: 0.9437\n",
            "Epoch 86/100 | Val Accuracy: 0.9423\n",
            "Epoch 87/100 | Val Accuracy: 0.9382\n",
            "Epoch 88/100 | Val Accuracy: 0.9477\n",
            "Epoch 89/100 | Val Accuracy: 0.9457\n",
            "Epoch 90/100 | Val Accuracy: 0.9464\n",
            "Epoch 91/100 | Val Accuracy: 0.9470\n",
            "Epoch 92/100 | Val Accuracy: 0.9477\n",
            "Epoch 93/100 | Val Accuracy: 0.9464\n",
            "Epoch 94/100 | Val Accuracy: 0.9409\n",
            "Epoch 95/100 | Val Accuracy: 0.9504\n",
            "Epoch 96/100 | Val Accuracy: 0.9362\n",
            "Epoch 97/100 | Val Accuracy: 0.9389\n",
            "Epoch 98/100 | Val Accuracy: 0.9511\n",
            "Epoch 99/100 | Val Accuracy: 0.9470\n",
            "Epoch 100/100 | Val Accuracy: 0.9477\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_DIM = 128 * 9\n",
        "HIDDEN_DIM = 256\n",
        "NUM_CLASSES = 6\n",
        "\n",
        "model = BaselineMLP(inputDim=INPUT_DIM, hiddenDim=HIDDEN_DIM, numClasses=NUM_CLASSES).to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss() # appropriate for multi class\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()           # Clear old gradients\n",
        "        logits = model(X_batch)         # Forward pass\n",
        "        loss = criterion(logits, y_batch) # Calculate loss\n",
        "        loss.backward()                 # Backpropagation\n",
        "        optimizer.step()                # Update weights\n",
        "\n",
        "\n",
        "    # VALIDATION PHASE\n",
        "    model.eval()\n",
        "    val_preds, val_targets = [], []\n",
        "\n",
        "    with torch.no_grad(): # Don't track gradients during validation (saves memory)\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            logits = model(X_batch)\n",
        "\n",
        "            # Get the predicted class (highest probability)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_targets.extend(y_batch.numpy())\n",
        "\n",
        "    val_acc = accuracy_score(val_targets, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Val Accuracy: {val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eb5603",
      "metadata": {
        "id": "b8eb5603"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluating on Test Set\")\n",
        "model.eval()\n",
        "test_preds, test_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_targets.extend(y_batch.numpy())\n",
        "\n",
        "# Calculate final required metrics\n",
        "final_acc = accuracy_score(test_targets, test_preds)\n",
        "final_macro_f1 = f1_score(test_targets, test_preds, average='macro')\n",
        "\n",
        "print(f\"Final Test Accuracy: {final_acc:.4f}\")\n",
        "print(f\"Final Test Macro-F1: {final_macro_f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "workingenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}